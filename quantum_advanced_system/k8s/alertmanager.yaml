apiVersion: monitoring.coreos.com/v1
kind: Alertmanager
metadata:
  name: quantum-alertmanager
  namespace: monitoring
  labels:
    app: alertmanager
    component: alerts
spec:
  replicas: 3
  retention: 120h
  resources:
    requests:
      memory: 1Gi
      cpu: "500m"
    limits:
      memory: 2Gi
      cpu: "1"
  storage:
    volumeClaimTemplate:
      spec:
        storageClassName: standard
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 20Gi
  configSecret:
    name: alertmanager-config
---
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-config
  namespace: monitoring
type: Opaque
stringData:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
      smtp_smarthost: 'smtp.example.com:587'
      smtp_from: 'alerts@quantum-finance.com'
      smtp_auth_username: 'alert-user'
      smtp_auth_password: 'alert-password'
    route:
      group_by: ['alertname', 'severity']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 3h
      receiver: 'quantum-team'
      routes:
      - match:
          severity: 'critical'
        receiver: 'on-call'
    receivers:
    - name: 'quantum-team'
      email_configs:
      - to: 'quantum-team@example.com'
    - name: 'on-call'
      email_configs:
      - to: 'on-call@example.com'
      pagerduty_configs:
      - service_key: 'your-pagerduty-key'
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: quantum-alert-rules
  namespace: monitoring
  labels:
    role: alert-rules
    prometheus: quantum
spec:
  groups:
  - name: quantum-system
    rules:
    - alert: HighErrorRate
      expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.1
      for: 10m
      labels:
        severity: critical
      annotations:
        summary: "High error rate on {{ $labels.instance }}"
        description: "Error rate is {{ $value }} for service {{ $labels.service }}"
        
    - alert: QuantumEngineDown
      expr: up{job="quantum-engine"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Quantum Engine down on {{ $labels.instance }}"
        description: "Quantum Engine has been down for more than 5 minutes"
        
    - alert: HighQuantumLatency
      expr: histogram_quantile(0.99, sum(rate(quantum_engine_execution_time_seconds_bucket[5m])) by (le)) > 5
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: "High latency in quantum computations"
        description: "99th percentile latency is {{ $value }} seconds"
        
    - alert: DatabaseDown
      expr: up{job="postgres"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "PostgreSQL database down on {{ $labels.instance }}"
        description: "Database has been down for more than 5 minutes"